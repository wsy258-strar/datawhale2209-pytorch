{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f66a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dc7130",
   "metadata": {},
   "source": [
    "## 1.张量运算示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0d2aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "?torch.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "939e6ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(1) tensor(1, dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "#创建tensor，用dtype指定类型\n",
    "a = torch.tensor(1.0,dtype = torch.float)\n",
    "b = torch.tensor(1,dtype = torch.long)\n",
    "c = torch.tensor(1.0,dtype = torch.int8) #可以进行强制类型的转换\n",
    "print(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "794ad041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.3799e+29, 3.0645e-41, 4.3799e+29],\n",
      "        [3.0645e-41, 4.3799e+29, 3.0645e-41]]) \n",
      " tensor([1, 2, 3, 4], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "#使用制定类型的函数随机初始化指定大小的tensor\n",
    "d = torch.FloatTensor(2,3)\n",
    "e = torch.IntTensor([1,2,3,4])\n",
    "print(d,'\\n',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1305ee3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "#tensor和numpy array之间的相互转换\n",
    "import numpy as np\n",
    "g = np.array([[1,2,3],[4,5,6]])\n",
    "h = torch.tensor(g)\n",
    "print(h)\n",
    "i = torch.from_numpy(g)\n",
    "print(i)\n",
    "j = h.numpy()\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e410223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2013, 0.8582, 0.6249],\n",
      "        [0.1716, 0.1424, 0.2826]]) \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      " tensor([0, 2, 4, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "#构造Tensor的函数\n",
    "k = torch.rand(2,3)\n",
    "l = torch.ones(2,3)\n",
    "m = torch.zeros(2,3)\n",
    "n = torch.arange(0,10,2)\n",
    "print(k,'\\n',l,'\\n',m,'\\n',n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1256a0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "#查看tensor维度\n",
    "print(k.shape)\n",
    "print(k.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05931fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2013, 1.8582, 1.6249],\n",
      "        [1.1716, 1.1424, 1.2826]])\n"
     ]
    }
   ],
   "source": [
    "#tensor运算\n",
    "o = torch.add(k,l)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a481565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.8582, 1.1424])\n"
     ]
    }
   ],
   "source": [
    "#tensor索引方式与numpy类似\n",
    "print(o[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2ea3926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2013, 1.8582],\n",
      "        [1.6249, 1.1716],\n",
      "        [1.1424, 1.2826]])\n",
      "tensor([[1.2013, 1.8582],\n",
      "        [1.6249, 1.1716],\n",
      "        [1.1424, 1.2826]])\n"
     ]
    }
   ],
   "source": [
    "#改变tensor形状\n",
    "print(o.view(3,2))\n",
    "print(o.view(-1,2))  #-1:确定了其他维度，可以自动求出匹配的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fccd6cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "#tensor广播机制(注意维度)\n",
    "p = torch.arange(1,3).view(1,2)\n",
    "print(p)\n",
    "q = torch.arange(1,4).view(3,1)\n",
    "print(q)\n",
    "print(p+q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ff9b628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2013, 1.8582, 1.6249],\n",
      "        [1.1716, 1.1424, 1.2826]])\n",
      "tensor([[[1.2013, 1.8582, 1.6249]],\n",
      "\n",
      "        [[1.1716, 1.1424, 1.2826]]])\n",
      "torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "#扩展&压缩tensor的维度：squeeze\n",
    "print(o)\n",
    "r = o.unsqueeze(1)  #在第二个维度的位置，强行加上一个维度\n",
    "print(r)\n",
    "print(r.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27350a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2013, 1.8582, 1.6249],\n",
      "        [1.1716, 1.1424, 1.2826]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "s = r.squeeze(1) #注：squeeze的那一维必须是1才可以\n",
    "print(s)\n",
    "print(s.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562593c",
   "metadata": {},
   "source": [
    "## 2.自动求导示例\n",
    "- 通过函数y=x_1 + 2*x_2 来说明pytorch自动求导的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fe53b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.tensor(1.0,requires_grad = True) #支持可以被求导\n",
    "x2 = torch.tensor(2.0,requires_grad = True)\n",
    "y = x1+2*x2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f736f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(x1.requires_grad)\n",
    "print(x2.requires_grad)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ae4abb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_186/2855580583.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#查看每个导数的大小，此时应为还没有反向传播，因此导数不存在\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "#查看每个导数的大小，此时应为还没有反向传播，因此导数不存在\n",
    "print(x1.grad.data)\n",
    "print(x2.grad.data)\n",
    "print(y.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d145fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b97c43eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "#反向传播后看导数大小\n",
    "y = x1 + 2*x2\n",
    "y.backward()\n",
    "print(x1.grad.data)\n",
    "print(x2.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7169986e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# 导数实惠累积的，重复运行相同的命令。grad会增加\n",
    "y = x1 + 2*x2\n",
    "y.backward()\n",
    "print(x1.grad.data)\n",
    "print(x2.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39d7121",
   "metadata": {},
   "source": [
    "- 所以每次计算前需要清除当前的导数值避免积累，这一功能可以通过optimizer实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d2b2818",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_186/2963658061.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/openvivo_env_py37/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/openvivo_env_py37/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "#尝试，如果不允许求导，会出现什么情况\n",
    "x1 = torch.tensor(1.0,requires_grad = False) \n",
    "x2 = torch.tensor(2.0,requires_grad = False)\n",
    "y = x1+2*x2\n",
    "y.backward() #出现报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c869f509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y #可以前向传播，不能反向传播"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
